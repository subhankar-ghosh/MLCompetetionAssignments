{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print( 'Train data shape: ', X_train.shape)\n",
    "print( 'Train labels shape: ', y_train.shape)\n",
    "print( 'Validation data shape: ', X_val.shape)\n",
    "print( 'Validation labels shape: ', y_val.shape)\n",
    "print( 'Test data shape: ', X_test.shape)\n",
    "print( 'Test labels shape: ', y_test.shape)\n",
    "print( 'dev data shape: ', X_dev.shape)\n",
    "print( 'dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.374939\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print( 'loss: %f' % loss)\n",
    "print( 'sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.652975 analytic: 0.652975, relative error: 1.000000e+00\n",
      "numerical: -0.188282 analytic: 0.188282, relative error: 1.000000e+00\n",
      "numerical: 0.040962 analytic: -0.040962, relative error: 1.000000e+00\n",
      "numerical: 0.384974 analytic: -0.384974, relative error: 1.000000e+00\n",
      "numerical: -1.739533 analytic: 1.739533, relative error: 1.000000e+00\n",
      "numerical: -6.243234 analytic: 6.243234, relative error: 1.000000e+00\n",
      "numerical: 0.960635 analytic: -0.960635, relative error: 1.000000e+00\n",
      "numerical: 0.738841 analytic: -0.738841, relative error: 1.000000e+00\n",
      "numerical: 1.371528 analytic: -1.371528, relative error: 1.000000e+00\n",
      "numerical: 1.427974 analytic: -1.427974, relative error: 1.000000e+00\n",
      "numerical: 0.881785 analytic: -0.838979, relative error: 1.000000e+00\n",
      "numerical: 1.307778 analytic: -1.294458, relative error: 1.000000e+00\n",
      "numerical: -2.075822 analytic: 2.031153, relative error: 1.000000e+00\n",
      "numerical: -1.422427 analytic: 1.445121, relative error: 1.000000e+00\n",
      "numerical: -1.992806 analytic: 2.024415, relative error: 1.000000e+00\n",
      "numerical: -3.204739 analytic: 3.224954, relative error: 1.000000e+00\n",
      "numerical: -0.577886 analytic: 0.627045, relative error: 1.000000e+00\n",
      "numerical: 1.272742 analytic: -1.218966, relative error: 1.000000e+00\n",
      "numerical: 0.626384 analytic: -0.617757, relative error: 1.000000e+00\n",
      "numerical: -2.148482 analytic: 2.177217, relative error: 1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.373656e+00 computed in 0.468739s\n",
      "[[ 0.07451632  0.12093111  0.14046382 ...,  0.08718157  0.04219207\n",
      "   0.07564283]\n",
      " [ 0.0958739   0.110622    0.05326094 ...,  0.06373946  0.11177794\n",
      "   0.10336354]\n",
      " [ 0.08494333  0.1292555   0.10189338 ...,  0.09876849  0.1321308\n",
      "   0.09153963]\n",
      " ..., \n",
      " [ 0.18394004  0.05828789  0.09826463 ...,  0.09838528  0.19973374\n",
      "   0.0736339 ]\n",
      " [ 0.11252308  0.04273064  0.03656785 ...,  0.08286723  0.25298893\n",
      "   0.03620912]\n",
      " [ 0.08851328  0.10214637  0.10769653 ...,  0.07360303  0.10900965\n",
      "   0.07515714]]\n",
      "vectorized loss: 2.373656e+00 computed in 0.031247s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print( 'naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print( 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print( 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print( 'Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 500: loss 14.161184\n",
      "iteration 100 / 500: loss 13.756142\n",
      "iteration 200 / 500: loss 14.929032\n",
      "iteration 300 / 500: loss 13.237215\n",
      "iteration 400 / 500: loss 16.418276\n",
      "training accuracy: 0.108265\n",
      "validation accuracy: 0.112000\n",
      "iteration 0 / 500: loss 14.608902\n",
      "iteration 100 / 500: loss 15.410476\n",
      "iteration 200 / 500: loss 14.714609\n",
      "iteration 300 / 500: loss 16.423372\n",
      "iteration 400 / 500: loss 16.661257\n",
      "training accuracy: 0.103510\n",
      "validation accuracy: 0.111000\n",
      "iteration 0 / 500: loss 14.938471\n",
      "iteration 100 / 500: loss 12.800420\n",
      "iteration 200 / 500: loss 13.182932\n",
      "iteration 300 / 500: loss 14.925286\n",
      "iteration 400 / 500: loss 14.165889\n",
      "training accuracy: 0.099531\n",
      "validation accuracy: 0.104000\n",
      "iteration 0 / 500: loss 12.769135\n",
      "iteration 100 / 500: loss 14.386530\n",
      "iteration 200 / 500: loss 13.888326\n",
      "iteration 300 / 500: loss 14.325208\n",
      "iteration 400 / 500: loss 13.508679\n",
      "training accuracy: 0.104122\n",
      "validation accuracy: 0.102000\n",
      "iteration 0 / 500: loss 13.253587\n",
      "iteration 100 / 500: loss 13.756305\n",
      "iteration 200 / 500: loss 12.908317\n",
      "iteration 300 / 500: loss 12.499911\n",
      "iteration 400 / 500: loss 13.565874\n",
      "training accuracy: 0.087490\n",
      "validation accuracy: 0.075000\n",
      "iteration 0 / 500: loss 16.002542\n",
      "iteration 100 / 500: loss 18.685989\n",
      "iteration 200 / 500: loss 18.080677\n",
      "iteration 300 / 500: loss 17.159385\n",
      "iteration 400 / 500: loss 19.050045\n",
      "training accuracy: 0.087143\n",
      "validation accuracy: 0.085000\n",
      "iteration 0 / 500: loss 31.369033\n",
      "iteration 100 / 500: loss 32.960829\n",
      "iteration 200 / 500: loss 31.638839\n",
      "iteration 300 / 500: loss 32.390633\n",
      "iteration 400 / 500: loss 30.207654\n",
      "training accuracy: 0.117735\n",
      "validation accuracy: 0.114000\n",
      "iteration 0 / 500: loss 167.516158\n",
      "iteration 100 / 500: loss 167.120836\n",
      "iteration 200 / 500: loss 166.566708\n",
      "iteration 300 / 500: loss 167.661071\n",
      "iteration 400 / 500: loss 167.084993\n",
      "training accuracy: 0.081673\n",
      "validation accuracy: 0.073000\n",
      "iteration 0 / 500: loss 1556.803592\n",
      "iteration 100 / 500: loss 1554.037109\n",
      "iteration 200 / 500: loss 1551.087207\n",
      "iteration 300 / 500: loss 1551.725865\n",
      "iteration 400 / 500: loss 1544.732139\n",
      "training accuracy: 0.115633\n",
      "validation accuracy: 0.120000\n",
      "iteration 0 / 500: loss 15461.102432\n",
      "iteration 100 / 500: loss 15156.309577\n",
      "iteration 200 / 500: loss 14855.900733\n",
      "iteration 300 / 500: loss 14561.275966\n",
      "iteration 400 / 500: loss 14274.234287\n",
      "training accuracy: 0.075041\n",
      "validation accuracy: 0.079000\n",
      "iteration 0 / 500: loss 17.196609\n",
      "iteration 100 / 500: loss 15.428229\n",
      "iteration 200 / 500: loss 11.440013\n",
      "iteration 300 / 500: loss 11.790899\n",
      "iteration 400 / 500: loss 15.192335\n",
      "training accuracy: 0.139592\n",
      "validation accuracy: 0.136000\n",
      "iteration 0 / 500: loss 18.525189\n",
      "iteration 100 / 500: loss 14.194249\n",
      "iteration 200 / 500: loss 13.765099\n",
      "iteration 300 / 500: loss 12.788009\n",
      "iteration 400 / 500: loss 11.392921\n",
      "training accuracy: 0.116714\n",
      "validation accuracy: 0.114000\n",
      "iteration 0 / 500: loss 11.684783\n",
      "iteration 100 / 500: loss 11.653059\n",
      "iteration 200 / 500: loss 10.562672\n",
      "iteration 300 / 500: loss 10.470472\n",
      "iteration 400 / 500: loss 10.314690\n",
      "training accuracy: 0.154429\n",
      "validation accuracy: 0.159000\n",
      "iteration 0 / 500: loss 14.847100\n",
      "iteration 100 / 500: loss 15.568518\n",
      "iteration 200 / 500: loss 12.750488\n",
      "iteration 300 / 500: loss 12.509210\n",
      "iteration 400 / 500: loss 13.245398\n",
      "training accuracy: 0.119163\n",
      "validation accuracy: 0.130000\n",
      "iteration 0 / 500: loss 15.345503\n",
      "iteration 100 / 500: loss 13.608766\n",
      "iteration 200 / 500: loss 15.924532\n",
      "iteration 300 / 500: loss 13.829873\n",
      "iteration 400 / 500: loss 13.514452\n",
      "training accuracy: 0.118204\n",
      "validation accuracy: 0.108000\n",
      "iteration 0 / 500: loss 20.766797\n",
      "iteration 100 / 500: loss 14.816237\n",
      "iteration 200 / 500: loss 13.562232\n",
      "iteration 300 / 500: loss 12.390758\n",
      "iteration 400 / 500: loss 12.629951\n",
      "training accuracy: 0.116694\n",
      "validation accuracy: 0.103000\n",
      "iteration 0 / 500: loss 38.989893\n",
      "iteration 100 / 500: loss 33.621063\n",
      "iteration 200 / 500: loss 30.661960\n",
      "iteration 300 / 500: loss 30.159167\n",
      "iteration 400 / 500: loss 28.112887\n",
      "training accuracy: 0.125041\n",
      "validation accuracy: 0.122000\n",
      "iteration 0 / 500: loss 170.968460\n",
      "iteration 100 / 500: loss 162.242078\n",
      "iteration 200 / 500: loss 155.233548\n",
      "iteration 300 / 500: loss 150.897298\n",
      "iteration 400 / 500: loss 145.852446\n",
      "training accuracy: 0.133551\n",
      "validation accuracy: 0.141000\n",
      "iteration 0 / 500: loss 1564.792042\n",
      "iteration 100 / 500: loss 1121.293180\n",
      "iteration 200 / 500: loss 803.586581\n",
      "iteration 300 / 500: loss 576.966080\n",
      "iteration 400 / 500: loss 414.832802\n",
      "training accuracy: 0.162837\n",
      "validation accuracy: 0.150000\n",
      "iteration 0 / 500: loss 15507.724988\n",
      "iteration 100 / 500: loss 543.241259\n",
      "iteration 200 / 500: loss 26.074563\n",
      "iteration 300 / 500: loss 8.212204\n",
      "iteration 400 / 500: loss 7.580230\n",
      "training accuracy: 0.260878\n",
      "validation accuracy: 0.271000\n",
      "iteration 0 / 500: loss 15.349812\n",
      "iteration 100 / 500: loss 9.058461\n",
      "iteration 200 / 500: loss 8.703870\n",
      "iteration 300 / 500: loss 8.624059\n",
      "iteration 400 / 500: loss 8.085173\n",
      "training accuracy: 0.340878\n",
      "validation accuracy: 0.344000\n",
      "iteration 0 / 500: loss 14.499668\n",
      "iteration 100 / 500: loss 8.740168\n",
      "iteration 200 / 500: loss 8.374578\n",
      "iteration 300 / 500: loss 8.174069\n",
      "iteration 400 / 500: loss 8.084086\n",
      "training accuracy: 0.340796\n",
      "validation accuracy: 0.351000\n",
      "iteration 0 / 500: loss 17.625441\n",
      "iteration 100 / 500: loss 9.112649\n",
      "iteration 200 / 500: loss 8.903634\n",
      "iteration 300 / 500: loss 8.830397\n",
      "iteration 400 / 500: loss 8.242863\n",
      "training accuracy: 0.337327\n",
      "validation accuracy: 0.333000\n",
      "iteration 0 / 500: loss 18.136952\n",
      "iteration 100 / 500: loss 9.211252\n",
      "iteration 200 / 500: loss 8.393532\n",
      "iteration 300 / 500: loss 8.983806\n",
      "iteration 400 / 500: loss 8.695051\n",
      "training accuracy: 0.339041\n",
      "validation accuracy: 0.351000\n",
      "iteration 0 / 500: loss 13.362576\n",
      "iteration 100 / 500: loss 9.479021\n",
      "iteration 200 / 500: loss 9.113172\n",
      "iteration 300 / 500: loss 8.671312\n",
      "iteration 400 / 500: loss 8.295097\n",
      "training accuracy: 0.341571\n",
      "validation accuracy: 0.317000\n",
      "iteration 0 / 500: loss 18.140985\n",
      "iteration 100 / 500: loss 10.385238\n",
      "iteration 200 / 500: loss 9.971397\n",
      "iteration 300 / 500: loss 9.491725\n",
      "iteration 400 / 500: loss 9.331260\n",
      "training accuracy: 0.342245\n",
      "validation accuracy: 0.336000\n",
      "iteration 0 / 500: loss 29.132253\n",
      "iteration 100 / 500: loss 17.235145\n",
      "iteration 200 / 500: loss 12.581415\n",
      "iteration 300 / 500: loss 10.167358\n",
      "iteration 400 / 500: loss 8.949980\n",
      "training accuracy: 0.385122\n",
      "validation accuracy: 0.386000\n",
      "iteration 0 / 500: loss 167.421237\n",
      "iteration 100 / 500: loss 7.891380\n",
      "iteration 200 / 500: loss 7.332749\n",
      "iteration 300 / 500: loss 7.409572\n",
      "iteration 400 / 500: loss 7.330570\n",
      "training accuracy: 0.358898\n",
      "validation accuracy: 0.364000\n",
      "iteration 0 / 500: loss 1551.231444\n",
      "iteration 100 / 500: loss 7.585959\n",
      "iteration 200 / 500: loss 7.411123\n",
      "iteration 300 / 500: loss 7.509249\n",
      "iteration 400 / 500: loss 7.512802\n",
      "training accuracy: 0.269061\n",
      "validation accuracy: 0.279000\n",
      "iteration 0 / 500: loss 15548.079286\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 8\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Personal Work\\Kaggle\\winter1516_assignment1\\assignment1\\cs231n\\classifiers\\softmax.py:89: RuntimeWarning: divide by zero encountered in log\n",
      "  loss=np.mean(-np.log( np.exp(z_correct)/np.sum(np.exp(z)) ))\n",
      "D:\\Personal Work\\Kaggle\\winter1516_assignment1\\assignment1\\cs231n\\classifiers\\softmax.py:93: RuntimeWarning: invalid value encountered in true_divide\n",
      "  z/=np.sum(z,axis=1).reshape(N,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.976081\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.515998\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.451687\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 12.719970\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 17.109263\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 3\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.425937\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 27.386385\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 3\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 170.235704\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 1568.174574\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15236.249804\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 17.638081\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.756913\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 13.615899\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.348168\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 13.528488\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.956337\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 30.969047\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 174.129413\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 1547.667779\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15494.288528\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.907883\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.964713\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 16.681245\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.069540\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.948952\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.728959\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 35.165531\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 168.735624\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 1544.973383\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15568.431211\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.402289\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 20.590165\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.711366\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.168008\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 12.964396\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 16.233312\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 30.549827\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 169.803684\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 1558.368077\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15285.879454\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 12.932208\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 12.349090\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 13.072248\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 16.010234\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.029114\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 23.222548\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 34.673476\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 169.269128\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 1556.441003\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15499.719260\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.737785\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.919742\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.875383\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 13.744181\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.414821\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.452475\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 30.574894\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 170.191648\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 1545.410392\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15394.616733\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.997025\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 14.719906\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 13.438405\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15.376945\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 17.623517\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 16.093981\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 30.547851\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 166.386429\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 1561.843438\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "iteration 0 / 500: loss 15394.520460\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]] 2\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "training accuracy: 0.100265\n",
      "validation accuracy: 0.087000\n",
      "lr 1.000000e-10 reg 1.000000e-03 train accuracy: 0.108265 val accuracy: 0.112000\n",
      "lr 1.000000e-10 reg 1.000000e-02 train accuracy: 0.103510 val accuracy: 0.111000\n",
      "lr 1.000000e-10 reg 1.000000e-01 train accuracy: 0.099531 val accuracy: 0.104000\n",
      "lr 1.000000e-10 reg 1.000000e+00 train accuracy: 0.104122 val accuracy: 0.102000\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.087490 val accuracy: 0.075000\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.087143 val accuracy: 0.085000\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.117735 val accuracy: 0.114000\n",
      "lr 1.000000e-10 reg 1.000000e+04 train accuracy: 0.081673 val accuracy: 0.073000\n",
      "lr 1.000000e-10 reg 1.000000e+05 train accuracy: 0.115633 val accuracy: 0.120000\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.075041 val accuracy: 0.079000\n",
      "lr 1.668101e-08 reg 1.000000e-03 train accuracy: 0.139592 val accuracy: 0.136000\n",
      "lr 1.668101e-08 reg 1.000000e-02 train accuracy: 0.116714 val accuracy: 0.114000\n",
      "lr 1.668101e-08 reg 1.000000e-01 train accuracy: 0.154429 val accuracy: 0.159000\n",
      "lr 1.668101e-08 reg 1.000000e+00 train accuracy: 0.119163 val accuracy: 0.130000\n",
      "lr 1.668101e-08 reg 1.000000e+01 train accuracy: 0.118204 val accuracy: 0.108000\n",
      "lr 1.668101e-08 reg 1.000000e+02 train accuracy: 0.116694 val accuracy: 0.103000\n",
      "lr 1.668101e-08 reg 1.000000e+03 train accuracy: 0.125041 val accuracy: 0.122000\n",
      "lr 1.668101e-08 reg 1.000000e+04 train accuracy: 0.133551 val accuracy: 0.141000\n",
      "lr 1.668101e-08 reg 1.000000e+05 train accuracy: 0.162837 val accuracy: 0.150000\n",
      "lr 1.668101e-08 reg 1.000000e+06 train accuracy: 0.260878 val accuracy: 0.271000\n",
      "lr 2.782559e-06 reg 1.000000e-03 train accuracy: 0.340878 val accuracy: 0.344000\n",
      "lr 2.782559e-06 reg 1.000000e-02 train accuracy: 0.340796 val accuracy: 0.351000\n",
      "lr 2.782559e-06 reg 1.000000e-01 train accuracy: 0.337327 val accuracy: 0.333000\n",
      "lr 2.782559e-06 reg 1.000000e+00 train accuracy: 0.339041 val accuracy: 0.351000\n",
      "lr 2.782559e-06 reg 1.000000e+01 train accuracy: 0.341571 val accuracy: 0.317000\n",
      "lr 2.782559e-06 reg 1.000000e+02 train accuracy: 0.342245 val accuracy: 0.336000\n",
      "lr 2.782559e-06 reg 1.000000e+03 train accuracy: 0.385122 val accuracy: 0.386000\n",
      "lr 2.782559e-06 reg 1.000000e+04 train accuracy: 0.358898 val accuracy: 0.364000\n",
      "lr 2.782559e-06 reg 1.000000e+05 train accuracy: 0.269061 val accuracy: 0.279000\n",
      "lr 2.782559e-06 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.386000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.logspace(-10, 10, 10) #[1e-7, 5e-7]\n",
    "regularization_strengths =  np.logspace(-3, 6, 10) #[1e3, 5e4, 1e8]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                              num_iters=500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        tracc = (np.mean(y_train == y_train_pred))\n",
    "        print( 'training accuracy: %f' % tracc)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        valacc = np.mean(y_val == y_val_pred)\n",
    "        print( 'validation accuracy: %f' % valacc )\n",
    "        results[(lr,rs)]=(tracc, valacc)\n",
    "        if(best_val < valacc):\n",
    "            best_val = valacc\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print ('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print( 'best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "#lr 2.782559e-06 reg 1.000000e+03 train accuracy: 0.385122 val accuracy: 0.386000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.369000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print ('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20e80c714a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
